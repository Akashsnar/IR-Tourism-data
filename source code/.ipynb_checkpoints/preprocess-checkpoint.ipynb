{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ea2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "def isNumeric(subj):\n",
    "    try:\n",
    "        return float(subj)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def tweetdict():\n",
    "    listTweets = dict()\n",
    "    tweets = (line.strip('\\n') for line in open(\n",
    "        \"./assets/tweet_list.txt\", 'r', encoding='utf-8-sig'))\n",
    "    print(tweets)\n",
    "    for tweet in tweets:\n",
    "        key, value = tweet.split('\\t')\n",
    "        # print(key,value)\n",
    "        listTweets[key] = value\n",
    "    # print(listTweets)\n",
    "    return listTweets\n",
    "\n",
    "\n",
    "def importTweets(verbose=False):\n",
    "    tweet_list = dict()\n",
    "    # tweets = (line.strip('\\n') for line in open('./assets/tweet_list.txt', 'r', encoding='utf-8-sig'))\n",
    "    tweets = (line.strip('\\n') for line in open(\n",
    "        \"./assets/tweet_list.txt\", 'r', encoding='utf-8-sig'))\n",
    "\n",
    "    for tweet in tweets:\n",
    "        key, value = tweet.split('\\t')\n",
    "\n",
    "        # print(key,value)\n",
    "        tweet_list[key] = filterSentence(value, verbose)\n",
    "\n",
    "    return tweet_list\n",
    "\n",
    "\n",
    "def importQuery(query, verbose=False):\n",
    "\n",
    "    query_list = dict()\n",
    "\n",
    "    # with open('./assets/test_queries.txt', 'r') as file:\n",
    "    #     fileContents = file.read()\n",
    "\n",
    "    # queryCheck = fileContents.strip('\\n').split(\"\\n\")\n",
    "    qlist = query.split(\" \")\n",
    "\n",
    "    # print(qlist)\n",
    "\n",
    "    # print(queryCheck, \"fggd\")\n",
    "    current_tweet = 1\n",
    "    # for x in queryCheck:\n",
    "    #     print(x)\n",
    "    query_list[current_tweet] = filterSentence(query, verbose)\n",
    "    #     # query_list[current_tweet] = x\n",
    "    #     current_tweet += 1\n",
    "\n",
    "    print(query_list, \"dictionary list\")\n",
    "\n",
    "    return query_list\n",
    "\n",
    "\n",
    "def filterSentence(sentence, verbose=False):\n",
    "    edge_stopwords = ['n\\'t', '\\'d', 'http', 'https', '//', '...']\n",
    "\n",
    "    custom_stopwords = set(stopwords.words('english')).union((line.strip(\n",
    "        '\\r\\n') for line in open('./assets/stop_words.txt', 'r'))).union(edge_stopwords)\n",
    "\n",
    "    tokens = [ps.stem(word.lower()) for word in word_tokenize(sentence)\n",
    "              if word.lower() not in custom_stopwords and\n",
    "              word not in string.punctuation and\n",
    "              not isNumeric(word)]\n",
    "\n",
    "    if verbose:\n",
    "        print('\\n Testing string: \\n\\n\\t ' + sentence + '\\n')\n",
    "        print(' Tokenized:\\n')\n",
    "        print('\\t' + '[%s]' % ', '.join(map(str, tokens)) + '\\n')\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def buildIndex(documents, verbose=False):\n",
    "\n",
    "    inverted_index = dict()\n",
    "\n",
    "    word_idf = dict()\n",
    "\n",
    "    for index, document in documents.items():\n",
    "        for token in document:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = {}\n",
    "            if token in inverted_index and index not in inverted_index[token]:\n",
    "                inverted_index[token][index] = 1\n",
    "            elif index in inverted_index[token]:\n",
    "                inverted_index[token][index] += 1\n",
    "\n",
    "    for token, current_document in inverted_index.items():\n",
    "        total_occurence = 0\n",
    "        for document, occurence in current_document.items():\n",
    "            total_occurence += occurence\n",
    "\n",
    "        word_idf[token] = round(\n",
    "            math.log((len(documents) / total_occurence), 2), 3)\n",
    "\n",
    "    for token, document_info in inverted_index.items():\n",
    "        for document, occurence in document_info.items():\n",
    "            document_info[document] = occurence * word_idf[token]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\r Inverted Index\")\n",
    "        with open(\"invertedIndex.json\", \"w\") as outfile:\n",
    "            json.dump(inverted_index, outfile)\n",
    "        print(json.dumps(inverted_index, indent=2))\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "def lengthOfDocument(inverted_index, tweets, verbose=False):\n",
    "    document_lengths = dict()\n",
    "\n",
    "    for tweet_id, tweet in tweets.items():\n",
    "        document_length = 0\n",
    "        for token in tweet:\n",
    "            document_length += pow(inverted_index[token][tweet_id], 2)\n",
    "\n",
    "        document_lengths[tweet_id] = round(math.sqrt(document_length), 3)\n",
    "\n",
    "    if verbose:\n",
    "        print('Length of documents', document_lengths)\n",
    "\n",
    "    return document_lengths\n",
    "\n",
    "\n",
    "def returnDocs():\n",
    "    listDocs = dict()\n",
    "    DocList = []\n",
    "    scoreList = []\n",
    "    scoreedDocs = (line.strip('\\n') for line in open(\n",
    "        \"./dist/Results.txt\", 'r', encoding='utf-8-sig'))\n",
    "\n",
    "    for docs in scoreedDocs:\n",
    "        docno = docs[1:19].strip()\n",
    "        score = docs[24:40].strip()\n",
    "        DocList.append(docno)\n",
    "        listDocs[docno] = score\n",
    "        scoreList.append(score)\n",
    "\n",
    "    return listDocs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59992358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
