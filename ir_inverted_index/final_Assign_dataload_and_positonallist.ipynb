{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5d478c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory C:\\Users\\akash\\IR Assignments\\ir_inverted_index\\th-dataset\n",
      "181 found\n",
      "Processing file 101.txt...\n",
      "Processing file 102.txt...\n",
      "Processing file 103.txt...\n",
      "Processing file 104.txt...\n",
      "Processing file 105.txt...\n",
      "Processing file 106.txt...\n",
      "Processing file 107.txt...\n",
      "Processing file 108.txt...\n",
      "Processing file 109.txt...\n",
      "Processing file 110.txt...\n",
      "Processing file 111.txt...\n",
      "Processing file 112.txt...\n",
      "Processing file 113.txt...\n",
      "Processing file 114.txt...\n",
      "Processing file 115.txt...\n",
      "Processing file 116.txt...\n",
      "Processing file 117.txt...\n",
      "Processing file 118.txt...\n",
      "Processing file 119.txt...\n",
      "Processing file 120.txt...\n",
      "Processing file 121.txt...\n",
      "Processing file 122.txt...\n",
      "Processing file 123.txt...\n",
      "Processing file 124.txt...\n",
      "Processing file 125.txt...\n",
      "Processing file 126.txt...\n",
      "Processing file 127.txt...\n",
      "Processing file 128.txt...\n",
      "Processing file 129.txt...\n",
      "Processing file 130.txt...\n",
      "Processing file 131.txt...\n",
      "Processing file 132.txt...\n",
      "Processing file 133.txt...\n",
      "Processing file 134.txt...\n",
      "Processing file 135.txt...\n",
      "Processing file 136.txt...\n",
      "Processing file 137.txt...\n",
      "Processing file 138.txt...\n",
      "Processing file 139.txt...\n",
      "Processing file 140.txt...\n",
      "Processing file 141.txt...\n",
      "Processing file 142.txt...\n",
      "Processing file 143.txt...\n",
      "Processing file 144.txt...\n",
      "Processing file 145.txt...\n",
      "Processing file 146.txt...\n",
      "Processing file 147.txt...\n",
      "Processing file 148.txt...\n",
      "Processing file 149.txt...\n",
      "Processing file 150.txt...\n",
      "Processing file 151.txt...\n",
      "Processing file 152.txt...\n",
      "Processing file 153.txt...\n",
      "Processing file 154.txt...\n",
      "Processing file 155.txt...\n",
      "Processing file 156.txt...\n",
      "Processing file 157.txt...\n",
      "Processing file 158.txt...\n",
      "Processing file 159.txt...\n",
      "Processing file 160.txt...\n",
      "Processing file 161.txt...\n",
      "Processing file 162.txt...\n",
      "Processing file 163.txt...\n",
      "Processing file 164.txt...\n",
      "Processing file 165.txt...\n",
      "Processing file 166.txt...\n",
      "Processing file 167.txt...\n",
      "Processing file 168.txt...\n",
      "Processing file 169.txt...\n",
      "Processing file 170.txt...\n",
      "Processing file 171.txt...\n",
      "Processing file 172.txt...\n",
      "Processing file 173.txt...\n",
      "Processing file 174.txt...\n",
      "Processing file 175.txt...\n",
      "Processing file 176.txt...\n",
      "Processing file 177.txt...\n",
      "Processing file 178.txt...\n",
      "Processing file 179.txt...\n",
      "Processing file 180.txt...\n",
      "Processing file 181.txt...\n",
      "Processing file 182.txt...\n",
      "Processing file 183.txt...\n",
      "Processing file 184.txt...\n",
      "Processing file 185.txt...\n",
      "Processing file 186.txt...\n",
      "Processing file 187.txt...\n",
      "Processing file 188.txt...\n",
      "Processing file 189.txt...\n",
      "Processing file 190.txt...\n",
      "Processing file 191.txt...\n",
      "Processing file 192.txt...\n",
      "Processing file 193.txt...\n",
      "Processing file 194.txt...\n",
      "Processing file 195.txt...\n",
      "Processing file 196.txt...\n",
      "Processing file 197.txt...\n",
      "Processing file 198.txt...\n",
      "Processing file 199.txt...\n",
      "Processing file 200.txt...\n",
      "Processing file 201.txt...\n",
      "Processing file 202.txt...\n",
      "Processing file 203.txt...\n",
      "Processing file 204.txt...\n",
      "Processing file 205.txt...\n",
      "Processing file 206.txt...\n",
      "Processing file 207.txt...\n",
      "Processing file 208.txt...\n",
      "Processing file 209.txt...\n",
      "Processing file 210.txt...\n",
      "Processing file 211.txt...\n",
      "Processing file 212.txt...\n",
      "Processing file 213.txt...\n",
      "Processing file 214.txt...\n",
      "Processing file 215.txt...\n",
      "Processing file 216.txt...\n",
      "Processing file 217.txt...\n",
      "Processing file 218.txt...\n",
      "Processing file 219.txt...\n",
      "Processing file 220.txt...\n",
      "Processing file 221.txt...\n",
      "Processing file 222.txt...\n",
      "Processing file 223.txt...\n",
      "Processing file 224.txt...\n",
      "Processing file 225.txt...\n",
      "Processing file 226.txt...\n",
      "Processing file 227.txt...\n",
      "Processing file 228.txt...\n",
      "Processing file 229.txt...\n",
      "Processing file 230.txt...\n",
      "Processing file 231.txt...\n",
      "Processing file 232.txt...\n",
      "Processing file 233.txt...\n",
      "Processing file 234.txt...\n",
      "Processing file 235.txt...\n",
      "Processing file 236.txt...\n",
      "Processing file 237.txt...\n",
      "Processing file 238.txt...\n",
      "Processing file 239.txt...\n",
      "Processing file 240.txt...\n",
      "Processing file 241.txt...\n",
      "Processing file 242.txt...\n",
      "Processing file 243.txt...\n",
      "Processing file 244.txt...\n",
      "Processing file 245.txt...\n",
      "Processing file 246.txt...\n",
      "Processing file 247.txt...\n",
      "Processing file 248.txt...\n",
      "Processing file 249.txt...\n",
      "Processing file 250.txt...\n",
      "Processing file 251.txt...\n",
      "Processing file 252.txt...\n",
      "Processing file 253.txt...\n",
      "Processing file 254.txt...\n",
      "Processing file 255.txt...\n",
      "Processing file 256.txt...\n",
      "Processing file 257.txt...\n",
      "Processing file 258.txt...\n",
      "Processing file 259.txt...\n",
      "Processing file 260.txt...\n",
      "Processing file 261.txt...\n",
      "Processing file 262.txt...\n",
      "Processing file 263.txt...\n",
      "Processing file 264.txt...\n",
      "Processing file 265.txt...\n",
      "Processing file 266.txt...\n",
      "Processing file 267.txt...\n",
      "Processing file 268.txt...\n",
      "Processing file 269.txt...\n",
      "Processing file 270.txt...\n",
      "Processing file 271.txt...\n",
      "Processing file 272.txt...\n",
      "Processing file 273.txt...\n",
      "Processing file 274.txt...\n",
      "Processing file 275.txt...\n",
      "Processing file 276.txt...\n",
      "Processing file 277.txt...\n",
      "Processing file 278.txt...\n",
      "Processing file 279.txt...\n",
      "Processing file 280.txt...\n",
      "Processing file 281.txt...\n",
      "Exporting results to Assign012019123Akash-output.txt....\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "from os import getcwd, path\n",
    "\n",
    "class PostingListItem:\n",
    "    def __init__(self, doc_id, freq) -> None:\n",
    "        self.doc_id = doc_id.split(\". \")[0]\n",
    "        self.freq = freq\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"({self.doc_id},{self.freq})\"\n",
    "\n",
    "class InvertedIndexer:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.posting = dict()\n",
    "\n",
    "\n",
    "    def process_word(self, word):\n",
    "\n",
    "        if \"“\" in word:\n",
    "            word = word.replace(\"“\", \"\")\n",
    "\n",
    "        if \"”\" in word:\n",
    "            word = word.replace(\"”\", \"\")\n",
    "        \n",
    "        if \"’\" in word:\n",
    "            word = word.replace(\"’\", \"\")\n",
    "\n",
    "        if \"‘\" in word:\n",
    "            word = word.replace(\"‘\", \"\")\n",
    " \n",
    "        if \"&\" in word:\n",
    "            word = word.replace(\"&\", \"\")        \n",
    "\n",
    "        if \"\\n\" in word:\n",
    "            word = word.replace(\"\\n\", \"\")\n",
    "\n",
    "        if \",\" in word:\n",
    "            word = word.split(\",\")[0]\n",
    "        \n",
    "        if \".\" in word:\n",
    "            word = word.replace(\".\", \"\")\n",
    "\n",
    "        if \"'\" in word:\n",
    "            word = word.replace(\"'\", \"\")\n",
    "        if \"(\" in word:\n",
    "            word=word.replace(\"(\",\"\")\n",
    "        if \")\" in word:\n",
    "            word=word.replace(\")\",\"\")\n",
    "        if \"-\" in word:\n",
    "            word=word.replace(\"-\",\"\")\n",
    "        \n",
    "        stemmer = PorterStemmer()\n",
    "        return stemmer.stem(word)\n",
    "    \n",
    "\n",
    "    def process_file(self, file_name, file_content):\n",
    "        print(f\"Processing file {file_name}...\")\n",
    "        document_term_map = dict()\n",
    "        words = file_content.split(\" \")\n",
    "\n",
    "        for word in words:\n",
    "            word = self.process_word(word)\n",
    "\n",
    "            if word in document_term_map.keys():\n",
    "                document_term_map[word] += 1\n",
    "            else:\n",
    "                document_term_map[word] = 1\n",
    "        \n",
    "        for item in document_term_map.items():\n",
    "            term, freq = item\n",
    "            posting_list_item = PostingListItem(file_name, freq)\n",
    "            if term in self.posting.keys():\n",
    "                self.posting[term].append(posting_list_item)\n",
    "            else:\n",
    "                self.posting[term] = list([posting_list_item])\n",
    "\n",
    "\n",
    "    def index_directory(self, dir_path):\n",
    "        files = [ f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "        print(f\"Processing directory {dir_path}\")\n",
    "        print(f\"{len(files)} found\")\n",
    "        for f in files:\n",
    "            file_content = open(join(dir_path, f), encoding='utf-8')\n",
    "            self.process_file(f, file_content.read())\n",
    "    \n",
    "    def export_index(self, output_file_name=\"Assign012019123Akash-output\"):\n",
    "        print(f\"Exporting results to {output_file_name}.txt....\")\n",
    "        for item in list(self.posting.items()):\n",
    "            key, value  = item\n",
    "            output_string = f\"{key} : \"\n",
    "            posting_list = \"\".join([f\"{str(posting_item.doc_id)},\" for posting_item in value])\n",
    "            output_string += f\"( {len(value)} , [{posting_list}])\\n\" \n",
    "            with open(f\"{output_file_name}.txt\", 'a', encoding=\"utf-8\") as f:\n",
    "                f.write(output_string)\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    MY_PATH = path.abspath(getcwd()) + r\"\\th-dataset\"\n",
    "\n",
    "    inverted_indexer = InvertedIndexer()\n",
    "    inverted_indexer.index_directory(dir_path=MY_PATH)\n",
    "\n",
    "    inverted_indexer.export_index()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57307c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
